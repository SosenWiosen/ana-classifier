{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from keras.src.layers import Dropout\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e3f43b8e49543d55",
   "metadata": {},
   "source": [
    "dst_path = \"/Users/sosen/UniProjects/eng-thesis/data/old/temp/CROPPED/NON-STED\"\n",
    "\n",
    "# Define parameters\n",
    "batch_size = 32\n",
    "\n",
    "img_height = 260\n",
    "img_width = 260"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d4b28632c4653829",
   "metadata": {},
   "source": [
    "# Load the Data\n",
    "train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory=dst_path,\n",
    "    labels='inferred',\n",
    "    subset=\"both\",\n",
    "    label_mode='categorical',\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    seed=123,\n",
    ")\n",
    "class_names = train_ds.class_names\n",
    "num_classes = len(class_names)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2fef72dbe3e95649",
   "metadata": {},
   "source": [
    "def copy_red_to_green_and_blue(image,label):\n",
    "    \"\"\"\n",
    "    This function takes an image and replaces the green and blue channels \n",
    "    with the values from the red channel.\n",
    "    \"\"\"\n",
    "    # Repeat the red channel across the RGB channels\n",
    "    # image[..., 0] is the red channel of the image\n",
    "    red_channel = image[..., 0:1]  # Extract only the red channel, shape (H, W, 1)\n",
    "    new_image = tf.concat([red_channel, red_channel, red_channel], axis=-1)\n",
    "    return new_image, label"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "426eba66e126921c",
   "metadata": {},
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "train_ds = train_ds.map(copy_red_to_green_and_blue)\n",
    "val_ds = val_ds.map(copy_red_to_green_and_blue)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "79ddc69db60964db",
   "metadata": {},
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def precision(y_true, y_pred):\n",
    "        '''\n",
    "        Precision metric. Only computes a batch-wise average of precision. Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        '''\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    def recall(y_true, y_pred):\n",
    "        '''\n",
    "        Recall metric. Only computes a batch-wise average of recall. Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        '''\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    y_pred = K.round(y_pred)\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "31b1427554c472",
   "metadata": {},
   "source": [
    "def normalize(x):\n",
    "    # mean = [0.485, 0.456, 0.406]\n",
    "    # std = [0.229, 0.224, 0.225]\n",
    "    mean = [0.24155269834305526,0.24155269834305526,0.24155269834305526]\n",
    "    std = [0.2809975193618667, 0.2809975193618667, 0.2809975193618667]\n",
    "    # mean = tf.constant([61.59593603488316, 61.59593603488316, 61.59593603488316])\n",
    "    # std = tf.constant([71.65436657360661, 71.65436657360661, 71.65436657360661])\n",
    "    return (x - mean) / std"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cb2612e877afcb54",
   "metadata": {},
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    # tf.keras.layers.Lambda(copy_red_to_green_and_blue),\n",
    "    # tf.keras.layers.Rescaling(1. / 255),  # Scale images to [0, 1]\n",
    "    # tf.keras.layers.Lambda(normalize),  # Normalize images\n",
    "    # tf.keras.layers.Rescaling(scale=2, offset=-1),  # Scale images to [-1, 1]\n",
    "    # You can add more augmentations if needed\n",
    "    # tf.keras.layers.RandomZoom(0.15),\n",
    "    # tf.keras.layers.RandomWidth(0.1),\n",
    "    # tf.keras.layers.RandomHeight(0.1),\n",
    "    # tf.keras.layers.RandomContrast(0.05),\n",
    "    tf.keras.layers.RandomRotation(0.1),\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomFlip(\"vertical\")\n",
    "], name=\"data_augmentation\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "46bace3ebba27d4a",
   "metadata": {},
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = layers.Input(shape=(img_height, img_width, 3))\n",
    "x = data_augmentation(inputs)\n",
    "\n",
    "model = tf.keras.applications.EfficientNetV2B0(weights='imagenet', include_top=False, input_tensor=x, pooling=None, include_preprocessing=True)\n",
    "model.trainable = False\n",
    "\n",
    "AVG_POOL_TOP = True\n",
    "if AVG_POOL_TOP:\n",
    "    # Rebuild top\n",
    "    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    top_dropout_rate = 0.2\n",
    "    x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\", name=\"pred\")(x)\n",
    "else:\n",
    "    x = Flatten()(model.output)\n",
    "    x = Dense(1000, activation='relu')(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Compile\n",
    "model = tf.keras.Model(inputs, outputs, name=\"EfficientNet\")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', f1])\n",
    "\n",
    "print(model.summary())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a4a2080b98a654ee",
   "metadata": {},
   "source": [
    "# Count occurrences of each class in the training dataset\n",
    "labels = np.concatenate([y for x, y in train_ds], axis=0)\n",
    "label_indices = np.argmax(labels, axis=1)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(label_indices),\n",
    "    y=label_indices\n",
    ")\n",
    "\n",
    "train_class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='f1',  # specify the F1 score for early stopping\n",
    "    patience=4,\n",
    "    mode='max',  #\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=20,\n",
    "    validation_data=val_ds,\n",
    "    class_weight=train_class_weights,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9be9b6f264e712b2",
   "metadata": {},
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Assuming model is already trained and compiled\n",
    "\n",
    "# Predict on validation data\n",
    "predictions = []\n",
    "y_true = []\n",
    "\n",
    "# Iterate over the validation dataset to collect true labels and predictions\n",
    "for images, labels in val_ds:\n",
    "    preds = model.predict(images)\n",
    "    predictions.extend(np.argmax(preds, axis=1))\n",
    "    y_true.extend(np.argmax(labels.numpy(), axis=1))  # Convert one-hot to integer labels\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "y_pred = np.array(predictions)\n",
    "y_true = np.array(y_true)\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2afe1bfadf3d1f51",
   "metadata": {},
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Normalize the confusion matrix by dividing each row (true condition) by the sum of the row\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plotting the normalized confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bf0903a98eb0fcd2",
   "metadata": {},
   "source": [
    "\n",
    "fig, axs = plt.subplots(3, figsize=(10, 15))\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "axs[0].plot(history.history['accuracy'])\n",
    "axs[0].plot(history.history['val_accuracy'])\n",
    "axs[0].set_title('Model Accuracy')\n",
    "axs[0].set_ylabel('Accuracy')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].legend(['Train', 'Val'], loc='upper left')\n",
    "axs[0].set_xticks(range(0, len(history.history['accuracy']), max(1, len(history.history['accuracy']) // 10)))\n",
    "\n",
    "# Plot training & validation loss values\n",
    "axs[1].plot(history.history['loss'])\n",
    "axs[1].plot(history.history['val_loss'])\n",
    "axs[1].set_title('Model Loss')\n",
    "axs[1].set_ylabel('Loss')\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].legend(['Train', 'Val'], loc='upper left')\n",
    "axs[1].set_xticks(range(0, len(history.history['loss']), max(1, len(history.history['loss']) // 10)))\n",
    "\n",
    "# Plot training & validation F1 score values\n",
    "axs[2].plot(history.history['f1'])\n",
    "axs[2].plot(history.history['val_f1'])\n",
    "axs[2].set_title('Model F1 Score')\n",
    "axs[2].set_ylabel('F1 Score')\n",
    "axs[2].set_xlabel('Epoch')\n",
    "axs[2].legend(['Train', 'Val'], loc='upper left')\n",
    "axs[2].set_xticks(range(0, len(history.history['f1']), max(1, len(history.history['f1']) // 10)))\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.show()\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2c449c6958d3a6fd",
   "metadata": {},
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def unfreeze_model(original_model):\n",
    "    # Clone the original model's architecture\n",
    "    # new_model = clone_model(original_model)\n",
    "    new_model = original_model\n",
    "    # Copy the weights from the original model to the new model\n",
    "    new_model.set_weights(original_model.get_weights())\n",
    "\n",
    "    # Unfreeze the top 10 layers, except for BatchNormalization layers\n",
    "    for layer in new_model.layers[-20:]:\n",
    "        if not isinstance(layer, layers.BatchNormalization):\n",
    "            layer.trainable = True\n",
    "\n",
    "    # Compile the new modeled with modified optimization settings\n",
    "    optimizer = Adam(learning_rate=1e-5)\n",
    "    new_model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=['accuracy', f1]  # ensure f1_metric is defined or imported\n",
    "    )\n",
    "\n",
    "    return new_model\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `model` is your pre-trained model\n",
    "# new_model = unfreeze_model(model)\n",
    "\n",
    "early_stopping_unfreeze = EarlyStopping(\n",
    "    monitor='f1',  # specify the F1 score for early stopping\n",
    "    patience=4,\n",
    "    mode='max',  #\n",
    "    restore_best_weights=True\n",
    ")\n",
    "epochs = 10  # @param {type: \"slider\", min:4, max:10}\n",
    "history_finetune = model.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=[early_stopping])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d3073a844155d6cd",
   "metadata": {},
   "source": [
    "fig, axs = plt.subplots(3, figsize=(10, 15))\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "axs[0].plot(history_finetune.history['accuracy'])\n",
    "axs[0].plot(history_finetune.history['val_accuracy'])\n",
    "axs[0].set_title('Model accuracy')\n",
    "axs[0].set_ylabel('Accuracy')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].legend(['Train', 'Val'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "axs[1].plot(history_finetune.history['loss'])\n",
    "axs[1].plot(history_finetune.history['val_loss'])\n",
    "axs[1].set_title('Model loss')\n",
    "axs[1].set_ylabel('Loss')\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].legend(['Train', 'Val'], loc='upper left')\n",
    "# Plot training & validation F1 score values\n",
    "axs[2].plot(history_finetune.history['f1'])\n",
    "axs[2].plot(history_finetune.history['val_f1'])\n",
    "axs[2].set_title('Model F1 Score')\n",
    "axs[2].set_ylabel('F1 Score')\n",
    "axs[2].set_xlabel('Epoch')\n",
    "axs[2].legend(['Train', 'Val'], loc='upper left')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "73f3683c40714c21",
   "metadata": {},
   "source": [
    "predictions = []\n",
    "y_true = []\n",
    "\n",
    "# Iterate over the validation dataset to collect true labels and predictions\n",
    "for images, labels in val_ds:\n",
    "    preds = model.predict(images)\n",
    "    predictions.extend(np.argmax(preds, axis=1))\n",
    "    y_true.extend(np.argmax(labels.numpy(), axis=1))  # Convert one-hot to integer labels\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "y_pred = np.array(predictions)\n",
    "y_true = np.array(y_true)\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1e18abdc41642dff",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
